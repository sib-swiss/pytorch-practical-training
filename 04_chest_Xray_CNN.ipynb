{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4005a579",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "![CNN representation from https://stanford.edu/~shervine/teaching/cs-230/](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-en.jpeg?3b7fccd728e29dc619e1bd8022bf71cf)\n",
    "\n",
    "For our example, we will use the [chest X-ray data-set](https://drive.google.com/file/d/1Y9iTkRrfh_2UfoG9o8CRjZc_3rj73nap/view)\n",
    "from [Kermany et al. 2018](https://www.sciencedirect.com/science/article/pii/S0092867418301545?via%3Dihub)\n",
    "\n",
    "| normal | pneumonia |\n",
    "| --- | --- |\n",
    "| <img alt='a sample normal chest x-ray from Kermany et al. 2018' src='images/NORMAL-1003233-0001.jpeg' width=\"400\"> | <img alt='a pneumonia normal chest x-ray from Kermany et al. 2018' src='images/BACTERIA-1008087-0001.jpeg' width=\"400\"> |\n",
    " \n",
    "\n",
    "\n",
    "If you need a refresher, you can use the excellent [CNN cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c07f7-0cdd-473f-994e-f6d15dd40289",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab, you will have to run the following line:\n",
    "#!pip install pytorch-model-summary\n",
    "#!wget https://github.com/Bjarten/early-stopping-pytorch/raw/refs/heads/main/early_stopping_pytorch/early_stopping.py\n",
    "#!mv early_stopping.py pytorchtools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5587e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_model_summary as pms \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bae0df",
   "metadata": {},
   "source": [
    "## data pre-processing \n",
    "\n",
    "The dataset contains \n",
    " * 1584 normal x-rays (1349 in train, 234 in validation) \n",
    " * 4274 pneumonia x-rays (3883 in train, 390 in validation)\n",
    "\n",
    "The original data-set is around 1.2GB in size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0117774-dd72-4a91-a340-79a67946ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab, you will have to run the following lines:\n",
    "\n",
    "## downloading 2 images:\n",
    "#!mkdir images\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/NORMAL-1003233-0001.jpeg -O images/NORMAL-1003233-0001.jpeg\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/BACTERIA-1008087-0001.jpeg -O images/BACTERIA-1008087-0001.jpeg\n",
    "\n",
    "## downloading an unzipping the full datasets (resized)\n",
    "#!wget https://sibcloud-my.sharepoint.com/:u:/g/personal/wandrille_duchemin_sib_swiss/ESDXFrlw6JJGiK8X7xG4aVEB06cxW82KyK_KWXMwccIVhw?download=1 -O chest_xray_224.zip\n",
    "#!wget https://sibcloud-my.sharepoint.com/:u:/g/personal/wandrille_duchemin_sib_swiss/EZLYtPxO4dlPr-2kysgQFYQBA0iLG3fEWnwrnlwvoHbZwg?download=1 -O chest_xray_64.zip\n",
    "\n",
    "#!mkdir data\n",
    "#!unzip chest_xray_224.zip\n",
    "#!mv chest_xray_224 data\n",
    "\n",
    "#!unzip chest_xray_64.zip\n",
    "#!mv chest_xray_64 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "imgN = read_image( \"images/NORMAL-1003233-0001.jpeg\" )\n",
    "imgP = read_image( \"images/BACTERIA-1008087-0001.jpeg\" )\n",
    "\n",
    "imgN.shape , imgP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2 , figsize = (10,5))\n",
    "ax[0].imshow(imgN.numpy()[0])\n",
    "ax[1].imshow(imgP.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13218a33",
   "metadata": {},
   "source": [
    "As they are, each image has its own size, and width to height ratio, so we will need to resize them to a common size first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We use the transformers from torchvision.v2\n",
    "R = v2.Resize( size = (224,224) , antialias = True ) \n",
    "fig,ax = plt.subplots(1,2 , figsize = (10,5))\n",
    "ax[0].imshow(R(imgN).numpy()[0])\n",
    "ax[1].imshow(R(imgP).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = v2.Resize( size = (32,32) , antialias = True ) \n",
    "fig,ax = plt.subplots(1,2 , figsize = (10,5))\n",
    "ax[0].imshow(R(imgN).numpy()[0])\n",
    "ax[1].imshow(R(imgP).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739ca99",
   "metadata": {},
   "source": [
    "How much should we resize? \n",
    "\n",
    "Should we do other operations?\n",
    "\n",
    "In image-based applications, it is common to augment a data-set by randomly cropping, rotating, and flipping the  images or shifting their colors or contrast for example.\n",
    "\n",
    "This is done to make the model resilient to such changes (a picture of a cat should still be classified as a cat, even if the cat is upside-down and in a dark room).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The classical stuff for classification : flip, crop, resize rotate\n",
    "## see : https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py\n",
    "## for more details\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomRotation(45),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485], std=[0.229]),\n",
    "])\n",
    "out = transforms(imgN)\n",
    "\n",
    "plt.imshow( out.numpy()[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dee1a",
   "metadata": {},
   "source": [
    "In our case, all our images are fairly well framed x-ray shots and we expect that new images would be similarly well-behaved, so we are only going to apply a resizing and a transformation of the pixel values to floats.\n",
    "\n",
    "---\n",
    "\n",
    "For the resize, we will go with a 224x224 size, which is a drastic diminution but still enough to get a fairly good classifier (it will become clear why exactly these values in the next notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57125a9",
   "metadata": {},
   "source": [
    "\n",
    "The image dataset in organized in several folder: one per target category (here NORMAL and PNEUMONIA).\n",
    "\n",
    "This is a fairly typical way to organize things, so torchvision already has utilities to load this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0003f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Here in order to keep the github repo somewhat light we have not included the full resolution images in it\n",
    "##  but we still keep this time code to in order to showcase how it would behave:\n",
    "##\n",
    "# %%time\n",
    "# train = torchvision.datasets.ImageFolder(\"data/chest_xray/train/\" , \n",
    "#                                          loader = read_image)\n",
    "\n",
    "# sizes = []\n",
    "# for x,y in train:\n",
    "#     sizes.extend(x.shape[1:])\n",
    "# print('minimum size:', min(sizes))\n",
    "# print('maximum size:', max(sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd06f63",
   "metadata": {},
   "source": [
    "Just reading the dataset took ~33.6 s\n",
    "\n",
    "It becomes even worse when you add the resize transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bee513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# R = v2.Resize( size = (224,224) , antialias = True ) \n",
    "# train = torchvision.datasets.ImageFolder(\"data/chest_xray/train/\" , \n",
    "#                                          loader = read_image,\n",
    "#                                         transform = R)\n",
    "# sizes = []\n",
    "# for x,y in train:\n",
    "#     sizes.extend(x.shape[1:])\n",
    "# print('minimum size:', min(sizes))\n",
    "# print('maximum size:', max(sizes))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64d46a30",
   "metadata": {},
   "source": [
    "minimum size: 224\n",
    "maximum size: 224\n",
    "CPU times: user 3min 15s, sys: 1.64 s, total: 3min 17s\n",
    "Wall time: 49.4 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee6b53",
   "metadata": {},
   "source": [
    "You have to think that this would be done at each epoch, so it is not ideal for our performance.\n",
    "\n",
    "When the dataset is not too large, you could keep all the images in RAM as Tensors and forego this.\n",
    "\n",
    "Here, because we do not want to do any other transform that a simple resize, we have kept the resized images and use this as dataset (this serves the additional purpose of having the data in the github repo while keeping it light)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "##\n",
    "## code to read the full res-images, transform them and write the result to disk\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# R = v2.Resize( size = (224,224) , antialias = True ) \n",
    "# train = torchvision.datasets.ImageFolder(\"data/chest_xray/train/\" , \n",
    "#                                          loader = read_image,\n",
    "#                                         transform = R)\n",
    "\n",
    "# baseFolder = 'data/chest_xray_224/train/' \n",
    "\n",
    "# targets = [\"NORMAL\",'PNEUMONIA']\n",
    "# for t in targets:\n",
    "#     os.makedirs( baseFolder + t )\n",
    "# for i , XY in enumerate( train ) :\n",
    "#     x,y = XY\n",
    "    \n",
    "#     folder = baseFolder + targets[y]\n",
    "#     im = Image.fromarray(x.numpy()[0])\n",
    "#     im.save( folder + '/' + targets[y] + '_' + str(i) + \".jpeg\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ac49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# R = v2.Resize( size = (224,224) , antialias = True ) \n",
    "# train = torchvision.datasets.ImageFolder(\"data/chest_xray/test/\" , \n",
    "#                                          loader = read_image,\n",
    "#                                         transform = R)\n",
    "\n",
    "# baseFolder = 'data/chest_xray_224/test/' \n",
    "\n",
    "# targets = [\"NORMAL\",'PNEUMONIA']\n",
    "# for t in targets:\n",
    "#     os.makedirs( baseFolder + t )\n",
    "# for i , XY in enumerate( train ) :\n",
    "#     x,y = XY\n",
    "    \n",
    "#     folder = baseFolder + targets[y]\n",
    "#     im = Image.fromarray(x.numpy()[0])\n",
    "#     im.save( folder + '/' + targets[y] + '_' + str(i) + \".jpeg\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = torchvision.datasets.ImageFolder(\"data/chest_xray_224/train/\" , \n",
    "                                         loader = read_image)\n",
    "\n",
    "sizes = []\n",
    "for x,y in tmp:\n",
    "    sizes.extend(x.shape[1:])\n",
    "print('minimum size:', min(sizes))\n",
    "print('maximum size:', max(sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a201e",
   "metadata": {},
   "source": [
    "much faster \n",
    "\n",
    "And so, creating our datasets looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('data/chest_xray_224/train', \n",
    "                                         loader = read_image,\n",
    "                                         transform = v2.ToDtype(torch.float32),\n",
    "                                         target_transform = lambda x : torch.Tensor([x])  )\n",
    "valid_dataset = torchvision.datasets.ImageFolder('data/chest_xray_224/test', \n",
    "                                         loader = read_image,\n",
    "                                         transform = v2.ToDtype(torch.float32),\n",
    "                                         target_transform = lambda x : torch.Tensor([x])   )\n",
    "    \n",
    "    \n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader( dataset= train_dataset , shuffle=True , batch_size = batch_size )\n",
    "valid_dataloader = DataLoader( dataset= valid_dataset , shuffle=True , batch_size = batch_size )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667250a",
   "metadata": {},
   "source": [
    "**Question:** why do we use `shuffle = True` here ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9672c",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "first the classical definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer ,  echo = True , echo_batch = False):\n",
    "    \n",
    "    size = len(dataloader.dataset) # how many batches do we have\n",
    "    model.train() #     Sets the module in training mode.\n",
    "    \n",
    "    for batch, (X,y) in enumerate(dataloader): # for each batch\n",
    "        X = X.to(device) # send the data to the GPU or whatever device you use for training\n",
    "        y = y.to(device) # send the data to the GPU or whatever device you use for training\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)              # prediction for the model -> forward pass\n",
    "        loss = loss_fn(pred, y)      # loss function from these prediction        \n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()              # backward propagation \n",
    "        #                            https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "        #                            https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "        \n",
    "        optimizer.step()             \n",
    "        optimizer.zero_grad()        # reset the gradients\n",
    "                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "\n",
    "        if echo_batch:\n",
    "            current =  (batch) * dataloader.batch_size +  len(X)\n",
    "            print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    if echo:\n",
    "        print(f\"Train loss: {loss.item():>7f}\")\n",
    "\n",
    "    # return the last batch loss\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dataloader, model, loss_fn, echo = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #     Sets the module in evaluation mode\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()  ## accumulating the loss function over the batches\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "\n",
    "    if echo:\n",
    "        print(f\"Valid Error: {valid_loss:>8f}\")\n",
    "    ## return the average loss / batch\n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b409",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Our model will chain a convolutional element with a more classical classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size = 224,\n",
    "                       channel_numbers=1 , \n",
    "                       kernel_size= 16, \n",
    "                       stride = 4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = channel_numbers , \n",
    "                      out_channels = 2, \n",
    "                      kernel_size = kernel_size, \n",
    "                      stride= stride ),\n",
    "            nn.ReLU(True), # inplace ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(in_channels = 2, out_channels = 4, kernel_size=5, stride=1),\n",
    "            nn.ReLU(True), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        ###\n",
    "        flatten_output_dim = 10 ## FIND THIS VALUE\n",
    "        ###\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flatten_output_dim, 16),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv(x)\n",
    "\n",
    "        out = self.flatten(out)\n",
    "\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = CNN(input_size = 224,\n",
    "            channel_numbers=1 , \n",
    "            kernel_size= 16, \n",
    "            stride = 4).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b245aa4",
   "metadata": {},
   "source": [
    "**Exercise:** Find the value `flatten_output_dim` : the dimension of the elements getting out of the convolutional layer (after flattening).\n",
    "\n",
    "If you have the correct size, the following cell will run without issue.\n",
    "\n",
    "> hint : \n",
    "> - 2D convolutional layer:\n",
    ">     - [concept](https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#convolution)\n",
    ">     - [pytorch doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    ">\n",
    "> - 2D max pool layer\n",
    ">     - [concept](https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#pooling)\n",
    ">     - [pytorch doc](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6e202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = train_dataset[0]\n",
    "print(pms.summary(model, x.reshape(1,1,224,224).to(device) , show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f807e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CNN_flatten_output_dim.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7fe48",
   "metadata": {},
   "source": [
    "Let's get a batch to check that we can compute our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (X,y) in enumerate(train_dataloader): # for each batch\n",
    "    print(batch , X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X.to(device))\n",
    "    print(pred.shape)\n",
    "    print( 'avg loss:', loss( pred , y.to(device) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we also define this to easily get accuracy \n",
    "def get_model_accuracy(model, dataloader):\n",
    "    Ys = np.array([] , dtype = 'float32' )\n",
    "    Ps = np.array([], dtype = 'float32' )\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            Ys = np.concatenate([Ys, y.squeeze().cpu().numpy()])\n",
    "            Ps = np.concatenate([Ps, (pred>0.5).squeeze().cpu().numpy()])\n",
    "\n",
    "    return np.mean( Ys == Ps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preamble -> define the model, the loss function, and the optimizer\n",
    "model = CNN(input_size = 224,\n",
    "            channel_numbers=1 , \n",
    "            kernel_size= 16, \n",
    "            stride = 4).to(device)\n",
    "\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = 5*10**-5 , momentum=0.9) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 10**-5 ) \n",
    "\n",
    "\n",
    "## container to keep the scores across all epochs\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "\n",
    "\n",
    "# overfitting can be an issue here. \n",
    "# we use the early stopping implemented in https://github.com/Bjarten/early-stopping-pytorch\n",
    "# initialize the early_stopping object. \n",
    "# patience: How long to wait after last time validation loss improved.\n",
    "early_stopping = EarlyStopping(patience=25, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c453de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## naive performance\n",
    "print( \"train accuracy:\", get_model_accuracy(model, train_dataloader) )\n",
    "print( \"valid accuracy:\", get_model_accuracy(model, valid_dataloader) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets do a single round, to learn how long it takes\n",
    "train_scores.append( train(train_dataloader, \n",
    "                           model, \n",
    "                           loss, \n",
    "                           optimizer, \n",
    "                           echo = True , echo_batch = True ) )\n",
    "\n",
    "valid_scores.append( valid(valid_dataloader, \n",
    "                           model, \n",
    "                           loss , \n",
    "                           echo = True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epoch):\n",
    "    echo = t%1==0\n",
    "    if echo:\n",
    "        print('Epoch',len(train_scores)+1 )    \n",
    "\n",
    "    train_scores.append( train(train_dataloader, \n",
    "                               model, \n",
    "                               loss, \n",
    "                               optimizer, \n",
    "                               echo = echo , echo_batch = False ) )\n",
    "\n",
    "    valid_scores.append( valid(valid_dataloader, \n",
    "                               model, \n",
    "                               loss , \n",
    "                               echo = echo) )\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(valid_scores[-1], model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_scores , label = 'train')\n",
    "plt.plot(valid_scores, label = 'validation')\n",
    "plt.axvline(np.argmin(valid_scores), linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('BCE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73659ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_scores , label = 'train')\n",
    "plt.plot(valid_scores, label = 'validation')\n",
    "plt.axvline(np.argmin(valid_scores), linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('BCE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb76c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"train accuracy:\", get_model_accuracy(model, train_dataloader) )\n",
    "print( \"valid accuracy:\", get_model_accuracy(model, valid_dataloader) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058edfb4",
   "metadata": {},
   "source": [
    "This is not stellar, but it is already fairly nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a9ed",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "\n",
    "We have used 224x224 images there. \n",
    "\n",
    "But maybe for this fairly simple task one could use much smaller images and still get an OK results?\n",
    "\n",
    "We have created a version of this dataset with 64x64 images, in `data/chest_xray_64/train`.\n",
    "\n",
    "create adapted dataloaders and adapt the model achitecture.\n",
    "\n",
    "For example, change the convolutional layer stride and kernel (now there is less pixels, so each pixel counts, so a smaller stride could me more appropriate).\n",
    "\n",
    "> In our experience, for a model trained for approximately 1 or 2 minutes, you should get performances slightly lower that with the 224x224 images but still respectable \n",
    "\n",
    "> Note: models I tried have much less parameters and run faster than the one with bigger image, so each epoch was faster and so I let them run for more epoch. I also needed to decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to create the 64x64 dataset\n",
    "##\n",
    "# %%time\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# S = 64\n",
    "# R = v2.Resize( size = (S,S) , antialias = True ) \n",
    "\n",
    "# for T in ['train','test']:\n",
    "#     dataset = torchvision.datasets.ImageFolder(\"data/chest_xray/\"+T+\"/\" , \n",
    "#                                              loader = read_image,\n",
    "#                                             transform = R)\n",
    "\n",
    "#     baseFolder = 'data/chest_xray_'+str(S)+'/'+T+'/' \n",
    "\n",
    "#     targets = [\"NORMAL\",'PNEUMONIA']\n",
    "#     for t in targets:\n",
    "#         os.makedirs( baseFolder + t ,exist_ok=True)\n",
    "#     for i , XY in enumerate( dataset ) :\n",
    "#         x,y = XY\n",
    "\n",
    "#         folder = baseFolder + targets[y]\n",
    "#         im = Image.fromarray(x.numpy()[0])\n",
    "#         im.save( folder + '/' + targets[y] + '_' + str(i) + \".jpeg\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "imgFull    = read_image( \"images/BACTERIA-1008087-0001.jpeg\" )\n",
    "imgReduced = read_image( \"data/chest_xray_64/train/PNEUMONIA/PNEUMONIA_1349.jpeg\" )\n",
    "\n",
    "fig,ax = plt.subplots(1,2 , figsize = (10,5))\n",
    "ax[0].imshow(imgFull.numpy()[0])\n",
    "ax[1].imshow(imgReduced.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48b55f-304b-4b01-a05c-c842e640b792",
   "metadata": {},
   "source": [
    "Steps:\n",
    " 1. create the dataloader\n",
    " 2. define the model object (you would need to adapt at least the `flatten_output_dim` now that the input image are not the same size)\n",
    " 3. define the loss function, and the optimizer (SGD with a learning rate of $10^{-5}$ and a momentum of 0.9 should be OK)\n",
    " 4. do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fb5ab-a938-4963-95d1-e12afc9bfbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b15021-f5ab-4f94-97ef-ff4716f3602a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369986e3",
   "metadata": {},
   "source": [
    "Creating the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-14 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5a84b",
   "metadata": {},
   "source": [
    "Defining the model with a parametric way of getting the `flatten_output_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc863f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 15-64 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd159f9",
   "metadata": {},
   "source": [
    "testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 65-79 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525ce6b",
   "metadata": {},
   "source": [
    "training preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9172bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 80-106 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7978142",
   "metadata": {},
   "source": [
    "Single round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 107-117 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27630ca8",
   "metadata": {},
   "source": [
    "training many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f56357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 118-147 solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fc856",
   "metadata": {},
   "source": [
    "model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6882192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 148- solutions/CNN_exercise_64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0683e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe334f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
