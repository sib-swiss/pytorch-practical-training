{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1aabe3",
   "metadata": {},
   "source": [
    "# a deep-learning classifier for subcellular localization of proteins\n",
    "\n",
    "This example is inspired from : https://github.com/vanessajurtz/lasagne4bio/blob/master/subcellular_localization/notebook%20tutorial/FFN.ipynb\n",
    "\n",
    "Based on the dataset from : https://academic.oup.com/bioinformatics/article/33/21/3387/3931857\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04014b37",
   "metadata": {},
   "source": [
    "## meet the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46270a08-ae84-46b9-9d2f-29a34e6207aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab, you will have to run the following line:\n",
    "#!pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be56664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict , Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_model_summary as pms \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79615e-0f07-4f14-904e-76259a8f860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab you will have to download \n",
    "# !wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/subcellular_localization/reduced_train.npz\n",
    "## and\n",
    "# !wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/subcellular_localization/reduced_val.npz\n",
    "\n",
    "## and adapt the following cell to open \"reduced_train.npz\" instead of 'data/subcellular_localization/reduced_train.npz'\n",
    "## as well as \"reduced_val.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce629cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2423, 400, 20)\n"
     ]
    }
   ],
   "source": [
    "# Load the encoded protein sequences, and labels \n",
    "train = np.load('data/subcellular_localization/reduced_train.npz' , allow_pickle=True)\n",
    "X_train = train['X_train']\n",
    "y_train = train['y_train']\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ab2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = np.load('data/subcellular_localization/reduced_val.npz')\n",
    "X_valid = validation['X_val']\n",
    "y_valid = validation['y_val']\n",
    "\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702db47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Nucleus',\n",
    "           'Cytoplasm',\n",
    "           'Extracellular',\n",
    "           'Mitochondrion',\n",
    "           'Cell membrane',\n",
    "           'ER',\n",
    "           'Chloroplast',\n",
    "           'Golgi apparatus',\n",
    "           'Lysosome',\n",
    "           'Vacuole']\n",
    "\n",
    "dico_classes_subcell={i:v for i,v in enumerate(classes)}\n",
    "\n",
    "for i in dico_classes_subcell.keys():\n",
    "    print('Target', i, dico_classes_subcell[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0dc7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7f0bdb2",
   "metadata": {},
   "source": [
    "Let's look at the target categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = [ dico_classes_subcell[y] for y in  y_train ] + [ dico_classes_subcell[y] for y in  y_valid ] ,\n",
    "              hue = ['train']*len(y_train) + ['val']*len(y_valid),\n",
    "              order = classes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181afa9b",
   "metadata": {},
   "source": [
    "Each sequence is encoded as a matrix where each position is a row of size 20, for each possible amino-acid.\n",
    "\n",
    "The values withing the matrix represent the amino acid frequency at the given position.\n",
    "\n",
    "Naturally, on one hand the proteins have different sizes, but on the other our neural network will require a fixed input size, thus, each position after the last contains only zeroes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d747f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f31b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( X_train[0,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,0:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0007b3",
   "metadata": {},
   "source": [
    "## Thinking about the problem, the loss, and the constraints it poses\n",
    "\n",
    "In the previous model we only had 2 classes in our target, thus our model output could merely be the probability  of belonging to class 1.\n",
    "Furthermore, our loss function was `nn.BCELoss`, which stands in for **Binary** Cross Entropy.\n",
    "\n",
    "However now our output target has 10 classes.\n",
    "\n",
    "Thus our output will have to be different, and our loss function will have to be as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e715d",
   "metadata": {},
   "source": [
    "\n",
    "**exercise**: With the help of the [CEloss doc](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), try to work out what the model output should be? how the target should be encoded? what parameters of the loss function may be of interest to us?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help : https://pytorch.org/docs/stable/tensors.html\n",
    "## this is how one can create a tensor of floats:\n",
    "\n",
    "example = torch.DoubleTensor([1,2,3])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEloss = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = ...\n",
    "\n",
    "target = ...\n",
    "\n",
    "CEloss(pred , target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6208184",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2630b39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d443f9a",
   "metadata": {},
   "source": [
    "making the loss work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a45f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-12 solutions/classifier_CEloss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01307c",
   "metadata": {},
   "source": [
    "additionnal considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 13-27 solutions/classifier_CEloss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 28- solutions/classifier_CEloss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a45f4",
   "metadata": {},
   "source": [
    "## build the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "X_train_tensor = torch.Tensor(X_train) \n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "# create your dataset\n",
    "train_dataset = TensorDataset(X_train_tensor,y_train_tensor) \n",
    "\n",
    "## creating a dataloader\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = batch_size ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "X_valid_tensor = torch.Tensor(X_valid) \n",
    "y_valid_tensor = torch.LongTensor(y_valid)\n",
    "\n",
    "# create your dataset\n",
    "valid_dataset = TensorDataset(X_valid_tensor,y_valid_tensor) \n",
    "\n",
    "## creating a dataloader\n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size = batch_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68248b58",
   "metadata": {},
   "source": [
    "## building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2858",
   "metadata": {},
   "source": [
    "Architecture:\n",
    " - flatten\n",
    " - hidden layer\n",
    "     - linear \n",
    "     - [dropout](https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html?highlight=dropout#dropout)\n",
    "     - ReLu activation\n",
    " - output layer \n",
    "     - linear \n",
    "     \n",
    "sizes:\n",
    " - input size = 8000\n",
    " - hidden size = [80]\n",
    " - output size = number of classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProteinLoc_neuralNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self , input_dim = 8000 , \n",
    "                         hidden_dim=[80] ,\n",
    "                         output_dim = 10 , \n",
    "                         dropout_fraction = 0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## we transform the input from 2D to 1D\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        elements = []\n",
    "        # each layer is made of a linear layer with a ReLu activation and a DropOut Layer\n",
    "        for i in range(len(hidden_dim)):\n",
    "            \n",
    "            elements.append( nn.Linear(input_dim, hidden_dim[i]) )\n",
    "            elements.append( nn.ReLU() )\n",
    "            elements.append( nn.Dropout(dropout_fraction) )\n",
    "            \n",
    "            input_dim = hidden_dim[i] ## update the input dimension for the next layer\n",
    "        \n",
    "        elements.append( nn.Linear(input_dim, output_dim) )\n",
    "\n",
    "        self.layers = nn.Sequential( *elements )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        ## NB: here, the output of the last layer are logits\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ProteinLoc_neuralNet( hidden_dim=[60,40,20]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pms.summary(model, torch.zeros(1,400,20).to(device), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "400*20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fc490",
   "metadata": {},
   "source": [
    "As we did before, playing a bit with the model cannot hurt : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "x, y = valid_dataset[:5] ## let's go with a batch of 5 samples\n",
    "\n",
    "with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = pred[0], y\n",
    "    print(f'Predicted proba: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from \"logits\" to probabilities\n",
    "nn.Softmax(dim=1)( pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the predicted category as the one with the highest score\n",
    "np.argmax(pred.cpu().detach(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a62f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEloss = nn.CrossEntropyLoss()\n",
    "CEloss(pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc3dfc",
   "metadata": {},
   "source": [
    "Now we can be reassured that our model does take the data as input, and output something we can compute a score on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891620b3",
   "metadata": {},
   "source": [
    "### defining training/validation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecafc56",
   "metadata": {},
   "source": [
    "Our cross entropy loss is all good and well, but it would be nice to be able to compute additionnal metrics while we train.\n",
    "\n",
    "Let's adapt our training function to reflect this\n",
    "\n",
    " * [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    " * [balanced_accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score)\n",
    " * [F1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight='balanced' , \n",
    "                                     classes = np.array(list( range(10) )), \n",
    "                                     y= y_train\n",
    "                                    )\n",
    "\n",
    "\n",
    "def get_additional_scores( predicted , target ):\n",
    "    sample_weights = class_weights[target]\n",
    "\n",
    "    return { 'balanced_accuracy' : metrics.balanced_accuracy_score( target , predicted , sample_weight=sample_weights ),\n",
    "             'accuracy' : metrics.accuracy_score( target , predicted ),\n",
    "             'f1' : metrics.f1_score( target , predicted ,\n",
    "                                     average = 'macro' ) }\n",
    "\n",
    "\n",
    "\n",
    "get_additional_scores( np.argmax(pred.detach().cpu().numpy(),axis=1) , y.cpu().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62870b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddebd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer , additional_score_function , echo = True , echo_batch = False):\n",
    "    \n",
    "    size = len(dataloader.dataset) # how many batches do we have\n",
    "    model.train() #     Sets the module in training mode.\n",
    "    \n",
    "    ## we will keep prediction and target on the whole dataset\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader): # for each batch\n",
    "        X, y = X.to(device), y.to(device) # send the data to the GPU or whatever device you use for training\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)              # prediction for the model -> forward pass\n",
    "        loss = loss_fn(pred, y)      # loss function from these prediction\n",
    "        \n",
    "        ## accumulate prediction and target on the whole dataset\n",
    "        all_predictions.extend( np.argmax(pred.detach().cpu().numpy() , axis=1) )\n",
    "        all_targets.extend( y.cpu().numpy() )\n",
    "        \n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()              # backward propagation \n",
    "        #                            https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "        #                            https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "        \n",
    "        optimizer.step()             \n",
    "        optimizer.zero_grad()        # reset the gradients\n",
    "                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \n",
    "        if echo_batch:\n",
    "            current =  (batch + 1) * len(X)\n",
    "            print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    if echo:\n",
    "        current =  (batch + 1) * len(X)\n",
    "        print(f\"Train loss: {loss.item():>7f}\")\n",
    "\n",
    "    \n",
    "    # return the last batch loss, as well as the metrics computed on all batches\n",
    "    scores = additional_score_function( all_predictions , all_targets )\n",
    "    scores['loss'] = loss.item()\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dataloader, model, loss_fn, additional_score_function, echo = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #     Sets the module in evaluation mode\n",
    "    \n",
    "    ## we will keep prediction and target on the whole dataset\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    valid_loss = 0\n",
    "    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()  ## accumulating the loss function over the batches\n",
    "            \n",
    "            ## accumulate prediction and target on the whole dataset\n",
    "            all_predictions.extend( np.argmax(pred.detach().cpu().numpy() , axis=1) )\n",
    "            all_targets.extend( y.cpu().numpy() )\n",
    "\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "\n",
    "    if echo:\n",
    "        print(f\"\\tValid loss: {valid_loss:>8f}\")\n",
    "    ## return the average loss / batch\n",
    "    scores = additional_score_function( all_predictions , all_targets )\n",
    "    scores['loss'] = valid_loss\n",
    "    return  scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b246c",
   "metadata": {},
   "source": [
    "## training the model\n",
    "\n",
    "Our optimizer will be [ADAM](https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e415a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preamble -> define the model, the loss function, and the optimizer\n",
    "model = ProteinLoc_neuralNet(input_dim = 8000 , \n",
    "                             hidden_dim=[80] ,\n",
    "                             output_dim = 10 , \n",
    "                             dropout_fraction = 0.1).to(device)\n",
    "\n",
    "\n",
    "W = torch.Tensor( compute_class_weight(class_weight='balanced' , \n",
    "                     classes = np.array( list( range(10) ) ), \n",
    "                     y= y_train) ).to(device)\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss(weight = W)\n",
    "print('weights_classes',W.cpu().numpy())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                       lr = 10**-4,\n",
    "                       weight_decay = 10**-2)\n",
    "\n",
    "\n",
    "## container to keep the scores across all epochs\n",
    "train_scores = defaultdict(list)\n",
    "valid_scores = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets do a single round, to learn how long it takes\n",
    "tmp_scores = train(train_dataloader, \n",
    "                   model, \n",
    "                   CEloss, \n",
    "                   optimizer, \n",
    "                   get_additional_scores , \n",
    "                   echo = True , echo_batch = True )\n",
    "for k in tmp_scores:\n",
    "    train_scores[k].append( tmp_scores[k] )\n",
    "\n",
    "\n",
    "\n",
    "tmp_scores = valid(valid_dataloader, \n",
    "                   model, \n",
    "                   CEloss , \n",
    "                   get_additional_scores ,\n",
    "                   echo = True)\n",
    "for k in tmp_scores:\n",
    "    valid_scores[k].append( tmp_scores[k] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cadf0b",
   "metadata": {},
   "source": [
    "From there, we can deduce approximately how much time training for 50, 100, or 500 epoch will take.\n",
    "\n",
    "Here: 192ms x 100 = 19.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "for t in range(1,epochs+1):\n",
    "\n",
    "    echo = t%10==0\n",
    "    if echo:\n",
    "        print('Epoch',t )    \n",
    "    \n",
    "    tmp_scores = train(train_dataloader, \n",
    "                       model, \n",
    "                       CEloss, \n",
    "                       optimizer, \n",
    "                       get_additional_scores , \n",
    "                       echo= echo , echo_batch=False )\n",
    "    for k in tmp_scores:\n",
    "        train_scores[k].append( tmp_scores[k] )\n",
    "\n",
    "\n",
    "\n",
    "    tmp_scores = valid(valid_dataloader, \n",
    "                       model, \n",
    "                       CEloss , \n",
    "                       get_additional_scores ,\n",
    "                       echo = echo)\n",
    "    for k in tmp_scores:\n",
    "        valid_scores[k].append( tmp_scores[k] )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize = (14,8))    \n",
    "\n",
    "\n",
    "for i,k in enumerate( ['loss', 'balanced_accuracy', 'accuracy', 'f1'] ) :\n",
    "\n",
    "    axes[i//2][i%2].plot(train_scores[k] , label = 'train')\n",
    "    axes[i//2][i%2].plot(valid_scores[k], label = 'validation')\n",
    "    axes[i//2][i%2].legend()\n",
    "    axes[i//2][i%2].set_xlabel('epoch')\n",
    "    axes[i//2][i%2].set_ylabel(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ae54f",
   "metadata": {},
   "source": [
    "Should we stop here ? Go for another 100 epochs? 500 epochs?\n",
    "\n",
    "We could use a [early stopping](https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html?highlight=dropout#early-stopping) mechanism, which stops iteration when the validation loss has not increased for `X` epochs (or when it has not improved by at least a given amount).\n",
    "\n",
    "This can be particularly important in cases where your model starts to grossly overfit the training data.\n",
    "\n",
    "For an implementation example, you can look up this [early stopping demo on MNIST](https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb), which we will use in the next notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ac122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model(X_valid_tensor.to(device))\n",
    "y_pred = np.argmax(y_pred.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "df = pd.crosstab( y_valid  , y_pred , rownames=['truth'] , colnames=['prediction'])\n",
    "df.columns = classes\n",
    "df.index = classes\n",
    "\n",
    "# simple heatmap\n",
    "#sns.heatmap(df , annot = True , fmt='.0f', cmap = 'viridis')\n",
    "\n",
    "#trick to make the 0s dissapear\n",
    "sns.heatmap(df , annot = df.astype(str).replace('0','') , fmt ='s' , cmap = 'viridis')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ff376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0602a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## playground\n",
    "\n",
    "what follows is just us playing around wit some code\n",
    "\n",
    "### effect of drop-out fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_ProteinLoc_neuralNet( DO , epochs , model = None , CEloss=None, optimizer=None ):\n",
    "    \n",
    "    if model is None:\n",
    "        ## preamble -> define the model, the loss function, and the optimizer\n",
    "        model = ProteinLoc_neuralNet(input_dim = 8000 , \n",
    "                                     hidden_dim=[80] ,\n",
    "                                     output_dim = 10 , \n",
    "                                     dropout_fraction = DO).to(device)\n",
    "\n",
    "\n",
    "        W = torch.Tensor( compute_class_weight(class_weight='balanced' , \n",
    "                             classes = np.arange(10) , \n",
    "                             y= y_train) ).to(device)\n",
    "\n",
    "        CEloss = nn.CrossEntropyLoss(weight = W)\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                               lr = 10**-4,\n",
    "                               weight_decay = 10**-2)\n",
    "\n",
    "\n",
    "    ## container to keep the scores across all epochs\n",
    "    train_scores = defaultdict(list)\n",
    "    valid_scores = defaultdict(list)\n",
    "\n",
    "    for t in range(1,epochs+1):\n",
    "\n",
    "        tmp_scores = train(train_dataloader, \n",
    "                           model, \n",
    "                           CEloss, \n",
    "                           optimizer, \n",
    "                           get_additional_scores , \n",
    "                           echo= False , echo_batch=False )\n",
    "        for k in tmp_scores:\n",
    "            train_scores[k].append( tmp_scores[k] )\n",
    "\n",
    "        tmp_scores = valid(valid_dataloader, \n",
    "                           model, \n",
    "                           CEloss , \n",
    "                           get_additional_scores ,\n",
    "                           echo = False)\n",
    "        for k in tmp_scores:\n",
    "            valid_scores[k].append( tmp_scores[k] )\n",
    "        \n",
    "    return train_scores,valid_scores , model, CEloss,optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results_dict = {'loss' :[], 'epoch':[],'drop_out':[], 'train_valid':[]}\n",
    "\n",
    "E=300\n",
    "for DO in np.arange(0,0.5,0.1):\n",
    "\n",
    "    train_scores,valid_scores ,_,_,_ = train_ProteinLoc_neuralNet( DO=DO , epochs=E)\n",
    "\n",
    "    results_dict['loss'].extend( train_scores['loss'] )\n",
    "    results_dict['epoch'].extend( range(1,E+1) )\n",
    "    results_dict['drop_out'].extend( [DO]*E )\n",
    "    results_dict['train_valid'].extend( ['train']*E )\n",
    "\n",
    "    results_dict['loss'].extend( valid_scores['loss'] )\n",
    "    results_dict['epoch'].extend( range(1,E+1) )\n",
    "    results_dict['drop_out'].extend( [DO]*E )\n",
    "    results_dict['train_valid'].extend( ['valid']*E )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06afe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(results_dict)\n",
    "df.drop_out = df.drop_out.astype(str)\n",
    "sns.lineplot( df , x='epoch' , y='loss' , hue = 'drop_out' , style = 'train_valid' , palette='viridis' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot( df.loc[df.train_valid=='valid',:] , x='epoch' , y='loss' , hue = 'drop_out' , style = 'train_valid' , palette='viridis' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c640fdb",
   "metadata": {},
   "source": [
    "### one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe10461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preamble -> define the model, the loss function, and the optimizer\n",
    "model2 = ProteinLoc_neuralNet(input_dim = 8000 , \n",
    "                             hidden_dim=[160,80,40,20] ,\n",
    "                             output_dim = 10 , \n",
    "                             dropout_fraction = 0.1).to(device)\n",
    "\n",
    "\n",
    "W = torch.Tensor( compute_class_weight(class_weight='balanced' , \n",
    "                     classes = np.arange(10) , \n",
    "                     y= y_train) ).to(device)\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss(weight = W)\n",
    "print('weights_classes',W.to('cpu').numpy())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), \n",
    "                       lr = 10**-5,\n",
    "                       weight_decay = 10**-2)\n",
    "\n",
    "\n",
    "## container to keep the scores across all epochs\n",
    "train_scores2 = defaultdict(list)\n",
    "valid_scores2 = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "for t in range(1,epochs+1):\n",
    "\n",
    "    echo = t%10==0\n",
    "    if echo:\n",
    "        print('Epoch',t )    \n",
    "    \n",
    "    tmp_scores = train(train_dataloader, \n",
    "                       model2, \n",
    "                       CEloss, \n",
    "                       optimizer, \n",
    "                       get_additional_scores , \n",
    "                       echo= echo , echo_batch=False )\n",
    "    for k in tmp_scores:\n",
    "        train_scores2[k].append( tmp_scores[k] )\n",
    "\n",
    "\n",
    "\n",
    "    tmp_scores = valid(valid_dataloader, \n",
    "                       model2, \n",
    "                       CEloss , \n",
    "                       get_additional_scores ,\n",
    "                       echo = echo)\n",
    "    for k in tmp_scores:\n",
    "        valid_scores2[k].append( tmp_scores[k] )\n",
    "print(\"Done!\")\n",
    "fig, axes = plt.subplots(2,2,figsize = (14,8))    \n",
    "\n",
    "\n",
    "for i,k in enumerate( ['loss', 'balanced_accuracy', 'accuracy', 'f1'] ) :\n",
    "\n",
    "    axes[i//2][i%2].plot(train_scores2[k] , label = 'train')\n",
    "    axes[i//2][i%2].plot(valid_scores2[k], label = 'validation')\n",
    "    axes[i//2][i%2].legend()\n",
    "    axes[i//2][i%2].set_xlabel('epoch')\n",
    "    axes[i//2][i%2].set_ylabel(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores2['accuracy'][-1] , valid_scores['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProteinLoc_neuralNet_ELU(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self , input_dim = 8000 , \n",
    "                         hidden_dim=[80] ,\n",
    "                         output_dim = 10 , \n",
    "                         dropout_fraction = 0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## we transform the input from 2D to 1D\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(  )\n",
    "        \n",
    "        # each layer is made of a linear layer with a ReLu activation and a DropOut Layer\n",
    "        for i in range(len(hidden_dim)):\n",
    "            \n",
    "            self.layers.append( nn.Linear(input_dim, hidden_dim[i]) )\n",
    "            self.layers.append( nn.ELU(1.0) )\n",
    "            self.layers.append( nn.Dropout(dropout_fraction) )\n",
    "            \n",
    "            input_dim = hidden_dim[i] ## update the input dimension for the next layer\n",
    "        \n",
    "        self.layers.append( nn.Linear(input_dim, output_dim) )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        ## NB: here, the output of the last layer are logits\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preamble -> define the model, the loss function, and the optimizer\n",
    "model3 = ProteinLoc_neuralNet_ELU(input_dim = 8000 , \n",
    "                             hidden_dim=[80] ,\n",
    "                             output_dim = 10 , \n",
    "                             dropout_fraction = 0.1).to(device)\n",
    "\n",
    "\n",
    "W = torch.Tensor( compute_class_weight(class_weight='balanced' , \n",
    "                     classes = np.arange(10) , \n",
    "                     y= y_train) )\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss(weight = W)\n",
    "print('weights_classes',W.numpy())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model3.parameters(), \n",
    "                       lr = 10**-4,\n",
    "                       weight_decay = 10**-2)\n",
    "\n",
    "\n",
    "## container to keep the scores across all epochs\n",
    "train_scores3 = defaultdict(list)\n",
    "valid_scores3 = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea50ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "for t in range(1,epochs+1):\n",
    "\n",
    "    echo = t%10==0\n",
    "    if echo:\n",
    "        print('Epoch',t )    \n",
    "    \n",
    "    tmp_scores = train(train_dataloader, \n",
    "                       model3, \n",
    "                       CEloss, \n",
    "                       optimizer, \n",
    "                       get_additional_scores , \n",
    "                       echo= echo , echo_batch=False )\n",
    "    for k in tmp_scores:\n",
    "        train_scores3[k].append( tmp_scores[k] )\n",
    "\n",
    "\n",
    "\n",
    "    tmp_scores = valid(valid_dataloader, \n",
    "                       model3, \n",
    "                       CEloss , \n",
    "                       get_additional_scores ,\n",
    "                       echo = echo)\n",
    "    for k in tmp_scores:\n",
    "        valid_scores3[k].append( tmp_scores[k] )\n",
    "print(\"Done!\")\n",
    "fig, axes = plt.subplots(2,2,figsize = (14,8))    \n",
    "\n",
    "\n",
    "for i,k in enumerate( ['loss', 'balanced_accuracy', 'accuracy', 'f1'] ) :\n",
    "\n",
    "    axes[i//2][i%2].plot(train_scores3[k] , label = 'train')\n",
    "    axes[i//2][i%2].plot(valid_scores3[k], label = 'validation')\n",
    "    axes[i//2][i%2].legend()\n",
    "    axes[i//2][i%2].set_xlabel('epoch')\n",
    "    axes[i//2][i%2].set_ylabel(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5271145",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores3['loss'][-1] , valid_scores2['loss'][-1] , valid_scores['loss'][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
