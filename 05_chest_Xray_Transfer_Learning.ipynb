{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec44ff6f",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "We will continue working on the chest x-ray dataset, from [Kermany et al. 2018](https://www.sciencedirect.com/science/article/pii/S0092867418301545?via%3Dihub)\n",
    " but this time we will stand closer to the original paper because we will use transfer learning.\n",
    "\n",
    "![fig 1 of \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\" by Kermany et al.](images/tranfer_learning_xray.jpg)\n",
    "\n",
    "The base model we will re-use is [ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "\n",
    "\n",
    "With inspirations from [this github repo](https://github.com/liyu95/Deep_learning_examples/blob/master/4.ResNet_X-ray_classification/Densenet_fine_tune.ipynb) and [this kaggle thread](https://www.kaggle.com/code/iamsdt/transferlearning-pytorch-resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157cf19-5e8e-4906-8d7c-de97ca6a3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab, you will have to run the following line:\n",
    "#!pip install pytorch-model-summary\n",
    "#!wget https://github.com/Bjarten/early-stopping-pytorch/raw/refs/heads/main/early_stopping_pytorch/early_stopping.py\n",
    "#!mv early_stopping.py pytorchtools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6e104-e08a-4965-979e-448004357129",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on google colab, you will have to run the following lines:\n",
    "\n",
    "## downloading example images:\n",
    "#!mkdir images\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/cat.jpg -O images/cat.jpg\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/car.jpg -O images/car.jpg\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/pandas_cat.jpg -O images/pandas_cat.jpg\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/NORMAL-1003233-0001.jpeg -O images/NORMAL-1003233-0001.jpeg\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/images/BACTERIA-1008087-0001.jpeg -O images/BACTERIA-1008087-0001.jpeg\n",
    "\n",
    "\n",
    "## downloading an unzipping the full datasets (resized)\n",
    "#!wget https://sibcloud-my.sharepoint.com/:u:/g/personal/wandrille_duchemin_sib_swiss/ESDXFrlw6JJGiK8X7xG4aVEB06cxW82KyK_KWXMwccIVhw?download=1 -O chest_xray_224.zip\n",
    "\n",
    "#!mkdir data\n",
    "#!unzip chest_xray_224.zip\n",
    "#!mv chest_xray_224 data\n",
    "\n",
    "## downloading embedded data\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/chest_Xray_train_embed.pt -O data/chest_Xray_train_embed.pt\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/chest_Xray_valid_embed.pt -O data/chest_Xray_valid_embed.pt\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/chest_Xray_train_y.pt -O data/chest_Xray_train_y.pt\n",
    "#!wget https://github.com/sib-swiss/pytorch-practical-training/raw/refs/heads/master/data/chest_Xray_valid_y.pt -O data/chest_Xray_valid_y.pt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082deaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_model_summary as pms \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7096cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Best available weights (currently alias for IMAGENET1K_V2)\n",
    "# Note that these weights may change across versions\n",
    "RN50_model = resnet50(weights=ResNet50_Weights.DEFAULT).to(device)\n",
    "RN50_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6effb2e",
   "metadata": {},
   "source": [
    "The inference transforms are available at `ResNet50_Weights.IMAGENET1K_V2.transforms` \n",
    "and perform the following preprocessing operations: \n",
    " * Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. \n",
    " * The images are resized to resize_size=[232] using interpolation=InterpolationMode.BILINEAR, \n",
    " * followed by a central crop of crop_size=[224]. \n",
    " * Finally the values are first rescaled to [0.0, 1.0] \n",
    " * then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_np_img( img ):\n",
    "    np_img = torch.moveaxis(img,0,2).numpy()    \n",
    "    m = np_img.min()\n",
    "    M = np_img.max()\n",
    "    r = M-m\n",
    "    return (np_img - m)/r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d51053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "RN50_transformer = ResNet50_Weights.IMAGENET1K_V2.transforms( antialias=True)\n",
    "\n",
    "cat = read_image( \"images/cat.jpg\" )\n",
    "cat_t = RN50_transformer(cat)\n",
    "\n",
    "fig,ax = plt.subplots(1,2 , figsize = (10,5))\n",
    "## Tensor are in C,H,W shape, but imshow wants H,W,C\n",
    "## so we move axis 0 to index 2\n",
    "ax[0].imshow( torch.moveaxis(cat,0,2).numpy())  \n",
    "ax[1].imshow( make_np_img( cat_t ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c448240",
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50_dategories = np.array( ResNet50_Weights.DEFAULT.meta['categories'] )\n",
    "RN50_dategories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49517899",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_t = cat_t.to(device)\n",
    "with torch.no_grad():\n",
    "    pred = RN50_model(cat_t.unsqueeze(0))\n",
    "    class_probas = pred.squeeze(0).softmax(0).cpu()\n",
    "    print( \"class proba shape\",class_probas.shape )\n",
    "    print( 'probas:    ', class_probas[:5] , '...' )\n",
    "    print( 'categories:',RN50_dategories[:5] , '...')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RN50_predict( img ):\n",
    "    img_t = RN50_transformer( img )\n",
    "\n",
    "    img_t = img_t.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = RN50_model(img_t.unsqueeze(0))\n",
    "        class_probas = pred.squeeze(0).softmax(0)\n",
    "    \n",
    "    order = np.argsort( class_probas.cpu().numpy()  )[::-1]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3 , figsize = (10,5))\n",
    "    ax[0].imshow(torch.moveaxis(img,0,2).cpu().numpy())\n",
    "    ax[0].set_title(RN50_dategories[order[0]])\n",
    "    ax[1].imshow( make_np_img( img_t.cpu() ) )\n",
    "    ax[1].set_title(RN50_dategories[order[0]])\n",
    "    \n",
    "    sns.barplot( x = class_probas.cpu().numpy()[order[:5]], \n",
    "             y = RN50_dategories[order[:5]] , ax = ax[2])\n",
    "    plt.tight_layout()\n",
    "RN50_predict( cat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50_predict( read_image(\"images/car.jpg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b36db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50_predict( read_image(\"images/pandas_cat.jpg\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4434a",
   "metadata": {},
   "source": [
    "Now, if we try it on our x-ray images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## force the image to be RGB\n",
    "img = read_image(\"images/NORMAL-1003233-0001.jpeg\" , \n",
    "                 mode=torchvision.io.ImageReadMode.RGB )\n",
    "\n",
    "RN50_predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image(\"images/BACTERIA-1008087-0001.jpeg\" , \n",
    "                 mode=torchvision.io.ImageReadMode.RGB )\n",
    "\n",
    "RN50_predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c5316",
   "metadata": {},
   "source": [
    "As expected, it does not make sense.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549fd3b",
   "metadata": {},
   "source": [
    "## setting up transfer-learning\n",
    "\n",
    "Rather than train a model from scratch as we did previously, we will leverage this complex model with [transfer learning](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks#parameter-tuning).\n",
    "\n",
    "So we will freeze most layers of resnet50 and plug our own classifier on top.\n",
    "\n",
    "![small Transfer Learning representation from https://stanford.edu/~shervine/teaching/cs-230/](https://stanford.edu/~shervine/teaching/cs-230/illustrations/transfer-learning-small-ltr.png?bee5e73de8fb2c6297a3a88804fabf5e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_t = RN50_transformer(cat).to(device)\n",
    "print(pms.summary(RN50_model, cat_t.unsqueeze(0) , show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we freeze all layers by setting their parameters .requires_grad=False\n",
    "for param in RN50_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the layer we want to replace is called fc\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "input_dim = RN50_model.fc.in_features\n",
    "\n",
    "RN50_model.fc = nn.Sequential(nn.Linear(input_dim , 1),\n",
    "                              nn.Sigmoid() ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_t = RN50_transformer(cat).to(device)\n",
    "print(pms.summary(RN50_model, cat_t.unsqueeze(0) , show_input=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f75b6a",
   "metadata": {},
   "source": [
    "## setup the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ca156",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_image_rgb = lambda x : read_image(x , mode=torchvision.io.ImageReadMode.RGB)\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('data/chest_xray_224/train', \n",
    "                                         loader = read_image_rgb,\n",
    "                                         transform = RN50_transformer,\n",
    "                                         target_transform = lambda x : torch.Tensor([x])  )\n",
    "valid_dataset = torchvision.datasets.ImageFolder('data/chest_xray_224/test', \n",
    "                                         loader = read_image_rgb,\n",
    "                                         transform = RN50_transformer,\n",
    "                                         target_transform = lambda x : torch.Tensor([x])   )\n",
    "    \n",
    "    \n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader( dataset= train_dataset , shuffle=True , batch_size = batch_size )\n",
    "valid_dataloader = DataLoader( dataset= valid_dataset , shuffle=True , batch_size = batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaeebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(train_dataloader))\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b324bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = RN50_model(X.to(device))\n",
    "    print(pred.shape)\n",
    "    print( 'avg loss:', loss( pred , y.to(device) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b72ae2",
   "metadata": {},
   "source": [
    "From there we would train the model as we usually do.\n",
    "\n",
    "However, in that particular case you can notice that just predicting 1 batch of 64 images took almost 4s on my measly CPU.\n",
    "\n",
    "So to go through the 5,858 images of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435102a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "((4/64)*5858)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88b995",
   "metadata": {},
   "source": [
    "I would need about 6 minutes.\n",
    "\n",
    "So I would have fairly long epochs, and that is without even counting the backward pass.\n",
    "\n",
    "Now, I could upgrade to a computer with a good GPU, but I could also apply reason like follow:\n",
    "\n",
    " * We have fixed all layers of Resnet50, expect the last\n",
    " * So at each epoch we compute the same thing, except for the last component\n",
    " * Going through all these layers is time consuming\n",
    " * Why not go though all these fixed layers once, and then train a model on this\n",
    " \n",
    "Basically, we would use a truncated Resnet50 model as a way to embed our 224x224 image (~50,176 pixels) into a 2048 feature space.\n",
    "\n",
    "And then we would train a classifier from that space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b646f1",
   "metadata": {},
   "source": [
    "**exercise** : modify the last layer so it does not modify the output and compute the embedding for 1 batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2d189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/TL_resnet50_embed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X,y = next(iter(train_dataloader))\n",
    "    \n",
    "    embedding = RN50_model(X.to(device))\n",
    "\n",
    "print( embedding.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cac68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf060c9e",
   "metadata": {},
   "source": [
    "---\n",
    "> What follow is the precomputation of the embeddings, which takes about 7min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/embed_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280f6e8",
   "metadata": {},
   "source": [
    "Then we read the embedding from files where we have saved them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_embedding = torch.load('data/chest_Xray_train_embed.pt')\n",
    "train_Ys = torch.load('data/chest_Xray_train_y.pt')\n",
    "valid_embedding = torch.load('data/chest_Xray_valid_embed.pt')\n",
    "valid_Ys = torch.load('data/chest_Xray_valid_y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbb563",
   "metadata": {},
   "source": [
    "## Exercise: fit the embedded data\n",
    "\n",
    "setup your dataloaders, create your model, and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf466e4b-2559-4026-bfd4-9dedd267d2be",
   "metadata": {},
   "source": [
    " * a couple of linear + ReLu layers do the trick to learn from the embedding \n",
    " * for the optimizer SGD with a learning rate of $10^{-2}$ worked in my tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf99c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1248a",
   "metadata": {},
   "source": [
    "setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-17 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4184b3",
   "metadata": {},
   "source": [
    "define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 18-44 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfd4fd",
   "metadata": {},
   "source": [
    "define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d804cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 45-111 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978ee48",
   "metadata": {},
   "source": [
    "training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 112-127 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d88d34",
   "metadata": {},
   "source": [
    "initial round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e27fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 128-140 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d6edb",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa53665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 141-171 solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94106c46",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 172- solutions/TL_model_embed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975698f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## merging the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "RN50_model.fc = copy.deepcopy( model.layers )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321ba89",
   "metadata": {},
   "source": [
    "## saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a443ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving \n",
    "torch.save(RN50_model, \"modified_RN50.model\") \n",
    "\n",
    "## loading\n",
    "RN50_model_loaded = torch.load( \"modified_RN50.model\" , weights_only=False) \n",
    "RN50_model_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8b8d7",
   "metadata": {},
   "source": [
    "## extra task: don't limit yourself to Neural Networks \n",
    "\n",
    "once you have the embedding you can apply whatever ML methodology you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b677eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_embedding.cpu().numpy()\n",
    "y_train = train_Ys.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = valid_embedding.cpu().numpy()\n",
    "y_valid = valid_Ys.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "x_pca = pca.fit_transform( X_train )\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d089af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x = x_pca[:,0] , y = x_pca[:,1] , hue = y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec68fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logi_r=LogisticRegression(solver = 'liblinear',n_jobs=1) \n",
    "\n",
    "pipeline_lr=Pipeline([('model',logi_r)])\n",
    "\n",
    "\n",
    "\n",
    "grid_values = {'model__C': np.logspace(-1,0,10),\n",
    "               'model__penalty': ['l1'] }\n",
    "\n",
    "grid_lr = GridSearchCV(pipeline_lr, \n",
    "                           param_grid = grid_values, \n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           n_jobs=1)\n",
    "grid_lr.fit( X_train , y_train )\n",
    "print('Grid best parameter (max. accuracy): ', grid_lr.best_params_)#get the best parameters\n",
    "print('Grid best score (accuracy): ', grid_lr.best_score_)#get the best score calculated from the train/validation dataset\n",
    "print('validation set score:' ,accuracy_score(y_valid , grid_lr.predict(X_valid)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238e0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
