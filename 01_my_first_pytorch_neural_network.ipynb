{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b752137e",
   "metadata": {},
   "source": [
    "# 2 toy datasets to discover pytorch \n",
    "\n",
    "This notebook will takes you through the step of creating and training a very simple neural network with pytorch in order to solve a simple binary classification task.\n",
    "\n",
    "It presumes that the user is already familiar with the theoretical concepts of deep-learning and instead focuses on their implementation in pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352be71f-71ac-4ca8-b848-0ec5c1f2f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you are on google colab you will have to run the following\n",
    "#!pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eab47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_model_summary as pms \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a55855",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non linearly separable data : the infamous Xor problem\n",
    "N = 10**2\n",
    "blob_X = np.concatenate( [np.random.randn(N,2)*0.1 + np.array([0,0]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([0,1]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([1,1]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([1,0])\n",
    "                    ])\n",
    "\n",
    "blob_y = np.array( [0]*N + [0]*N + [1]*N + [1]*N )\n",
    "\n",
    "## shuffling, for good measure:\n",
    "indexes = np.arange(blob_y.shape[0])\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(indexes)\n",
    "\n",
    "blob_X = blob_X[indexes,:]\n",
    "blob_y = blob_y[indexes]\n",
    "\n",
    "\n",
    "print('number of points:',len(blob_y))\n",
    "print('categories:',np.unique(blob_y))\n",
    "plt.scatter( blob_X[:,0],blob_X[:,1],c=blob_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f05679",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3bee9",
   "metadata": {},
   "source": [
    "## preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "tensor_X = torch.Tensor(blob_X) \n",
    "tensor_y = torch.Tensor(blob_y)\n",
    "\n",
    "# create your dataset\n",
    "full_dataset = TensorDataset(tensor_X,tensor_y) \n",
    "\n",
    "# split between train and validation datasets\n",
    "train_dataset, valid_dataset = random_split(full_dataset , [320,80] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b229893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cde3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len( train_dataset ) , len( valid_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a dataloader\n",
    "##   -> wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling \n",
    "##           and multiprocess data loading. \n",
    "##  not 100% needed here, but better to adopt best practices early on\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = 32 , shuffle = True ) \n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size = 32 , shuffle = True ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e39636",
   "metadata": {},
   "outputs": [],
   "source": [
    "len( train_dataloader ) , train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "next( iter( train_dataloader ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e662f",
   "metadata": {},
   "source": [
    "## shallow model\n",
    "\n",
    "<img src=\"images/shallow_LR.png\" alt=\"a neural network with no hidden layer\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc075447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression as a Pytorch module\n",
    "class LR_neuralNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self , input_dim = 2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( nn.Linear(input_dim, 1),          # Linear layer\n",
    "                                     nn.Sigmoid()  # Non-linear activation\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):  # Forward pass\n",
    "        proba = self.layers(x) \n",
    "        ## NB: here, the input  of the Sigmoid layer are logits\n",
    "        ##           the output of the Sigmoid layer are probas\n",
    "        return proba\n",
    "\n",
    "\n",
    "model = LR_neuralNet( input_dim = 2 ).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc26cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pms.summary(model , torch.zeros(1,2).to(device), show_input=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713033be",
   "metadata": {},
   "source": [
    "## preparing training, evaluating, ...\n",
    "\n",
    "### making a prediction and computing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b565694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "x, y = valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d738462",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = pred[0], y\n",
    "    print(f'Predicted proba: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9048ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd370e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cpu = pred.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4453ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred_cpu>0.5).numpy() == y.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred>0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred>0.5).float() == y.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391229e",
   "metadata": {},
   "source": [
    "> Without the Sigmoid layer what we would get here would be logits, which is often the case and actually expected by some loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16750db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will evaluate the prediction with the Binary Cross Entropy loss\n",
    "## https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#toc-entry-1\n",
    "##\n",
    "##  BCE = - (y*log(p) + (1-y)*log(1-p))\n",
    "\n",
    "loss = nn.BCELoss()(pred, y.unsqueeze(0).to(device)) # with BCE loss, we need to unsqueeze our target\n",
    "\n",
    "# equivalent to : - np.log(pred)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in practice this happens at the scale of a batch :\n",
    "x, y = next(iter(valid_dataloader))\n",
    "\n",
    "with torch.no_grad(): \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    pred = model(x)\n",
    "    print(f\"batch size: {y.shape[0]}\")\n",
    "    print( f'loss: {nn.BCELoss()(pred, y.unsqueeze(1)).item():>7f}' )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339bbfb",
   "metadata": {},
   "source": [
    "### defining training/validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer , echo = True):\n",
    "    \n",
    "    size = len(dataloader.dataset) # how many batches do we have\n",
    "    model.train() #     Sets the module in training mode.\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader): # for each batch\n",
    "        X, y = X.to(device), y.to(device) # send the data to the GPU or whatever device you use for training\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)              # prediction for the model -> forward pass\n",
    "        loss = loss_fn(pred.squeeze(), y)      # loss function from these prediction\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()              # backward propagation \n",
    "        #                            https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "        #                            https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "        \n",
    "        optimizer.step()             \n",
    "        optimizer.zero_grad()        # reset the gradients\n",
    "                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "\n",
    "        if echo:\n",
    "            current =  (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    # return the last batch loss:\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad387775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dataloader, model, loss_fn, echo = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #     Sets the module in evaluation mode\n",
    "    valid_loss, correct = 0, 0\n",
    "    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y.unsqueeze(1)).item()  ## accumulating the loss function over the batches\n",
    "            \n",
    "            correct += ((pred>0.5).float() == y.unsqueeze(1)).float().sum().item() ## counting number of true predictions\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    if echo:\n",
    "        print(f\"Valid Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "    ## return the average loss / batch\n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7828bf",
   "metadata": {},
   "source": [
    " ### actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ea660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of loss functions https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "# https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#toc-entry-1\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# optimizer https://pytorch.org/docs/stable/optim.html\n",
    "# https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#sgd\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 250\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_losses.append( train(train_dataloader, model, loss_fn, optimizer, echo=True) )\n",
    "    valid_losses.append( valid(valid_dataloader, model, loss_fn) )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc166a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from there we can continue training a bit\n",
    "train_losses.append( train(train_dataloader, model, loss_fn, optimizer, echo=False) )\n",
    "valid_losses.append( valid(valid_dataloader, model, loss_fn) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses , label = 'train')\n",
    "plt.plot(valid_losses, label = 'validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cross-entropy loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f266",
   "metadata": {},
   "source": [
    "## Non linearly separable data : the infamous Xor problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**2\n",
    "xor_X = np.concatenate( [np.random.randn(N,2)*0.1 + np.array([0,0]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([1,1]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([1,0]),\n",
    "                     np.random.randn(N,2)*0.1 + np.array([0,1])\n",
    "                    ])\n",
    "\n",
    "xor_y = np.array( [0]*N + [0]*N + [1]*N + [1]*N )\n",
    "\n",
    "## shuffling, for good measure:\n",
    "indexes = np.arange(xor_y.shape[0])\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(indexes)\n",
    "\n",
    "xor_X = xor_X[indexes,:]\n",
    "xor_y = xor_y[indexes]\n",
    "\n",
    "\n",
    "print('number of points:',len(xor_y))\n",
    "print('categories:',np.unique(xor_y))\n",
    "plt.scatter( xor_X[:,0],xor_X[:,1],c=xor_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "tensor_X = torch.Tensor(xor_X) \n",
    "tensor_y = torch.Tensor(xor_y)\n",
    "\n",
    "# create your dataset\n",
    "full_dataset = TensorDataset(tensor_X,tensor_y)\n",
    "\n",
    "# split between train and validation datasets\n",
    "train_dataset, valid_dataset = random_split(full_dataset , [320,80] )\n",
    "\n",
    "\n",
    "## creating a dataloader\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = 32 , shuffle = True ) \n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size = 32 , shuffle = True ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea805d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR_neuralNet( input_dim = 2 ).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 250\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "print(\"before training:\")\n",
    "valid(valid_dataloader, model, loss_fn , echo = True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_losses.append( train(train_dataloader, model, loss_fn, optimizer, echo=False) )\n",
    "    valid_losses.append( valid(valid_dataloader, model, loss_fn , echo = False) )\n",
    "print(\"Done!\")\n",
    "print(\"after training:\")\n",
    "valid(valid_dataloader, model, loss_fn , echo = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f207aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses , label = 'train')\n",
    "plt.plot(valid_losses, label = 'validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cross-entropy loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5,1.5,100),np.linspace(-0.5,1.5,100))\n",
    "\n",
    "pred = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]).to(device)).to(\"cpu\").detach().numpy()\n",
    "Z = pred.reshape(xx.shape)\n",
    "\n",
    "\n",
    "CS = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "cbar = plt.colorbar(CS, ax=ax)\n",
    "\n",
    "ax.scatter(xor_X[:,0],xor_X[:,1],c=xor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a5c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efe1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adcc00",
   "metadata": {},
   "source": [
    "We need something more complex.\n",
    "\n",
    "## Hands-on : from shallow to deep learning\n",
    "\n",
    "<img src=\"images/deep_LR.png\" alt=\"a neural network with 1 hidden layer\" style=\"width: 500px;\"/>\n",
    "\n",
    "Adapt the code below to add an additional layer:\n",
    "\n",
    " - make this layer of size 3 (ie, it is made of 3 neurons)\n",
    " - add a ReLU activation to this layer\n",
    "\n",
    "Instanciate this model, and then train it on the xor data\n",
    "\n",
    "**important info:**\n",
    " \n",
    " - relu layer class: `nn.ReLU()`. Like the Sigmoid activation, they do not take any argument on creation\n",
    " - you will likely need to train for at least 100 epoch\n",
    " - use the following optimizer `optimizer = torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.9) `\n",
    " - it may happen that your optimization fails for stochastic reasons. Do not hesitate to re-create and re-train your model a couple of times if that happens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class deepLR_neuralNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self , input_dim = 2 , hidden_dim = 3 ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( nn.Linear(input_dim, ... ),    # First Linear layer -> add the second parameter\n",
    "                                     ...                            # Hidden layer and ReLu\n",
    "                                     \n",
    "                                     nn.Sigmoid()  # Non-linear activation\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):  # Forward pass\n",
    "        proba = self.layers(x) \n",
    "        ## NB: here, the input  of the Sigmoid layer are logits\n",
    "        ##           the output of the Sigmoid layer are probas\n",
    "        return proba\n",
    "\n",
    "\n",
    "model = deepLR_neuralNet( input_dim = 2 , hidden_dim = 3  ).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110dcce-b067-428c-81b2-0529b2d10980",
   "metadata": {},
   "source": [
    "Below is the code to test your new class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c1834-1409-45cf-baca-6a8b4ca27b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check model input/output\n",
    "print(pms.summary(model, torch.zeros(1,2).to(device), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1b95f-3d1e-424e-9bed-8e423ff81406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model, loss and optimizer\n",
    "model = deepLR_neuralNet( input_dim = 2 , hidden_dim = 3 ).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1 , momentum = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e7abf-8a4e-41fd-b217-a3baf7733df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 250\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_losses.append( train(train_dataloader, model, loss_fn, optimizer, echo=False) )\n",
    "    valid_losses.append( valid(valid_dataloader, model, loss_fn , echo = False) )\n",
    "print(\"Done!\")\n",
    "plt.plot(train_losses , label = 'train')\n",
    "plt.plot(valid_losses, label = 'validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cross-entropy loss')\n",
    "valid(valid_dataloader, model, loss_fn , echo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b9f3b-cbda-49b5-af61-0669588bd229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89852adc",
   "metadata": {},
   "source": [
    "### correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89f216",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-18 solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afe634",
   "metadata": {},
   "source": [
    "check model input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19 solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1c7f1",
   "metadata": {},
   "source": [
    "define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7528595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 20-22 solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5ffa4",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6444878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 23-37 solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d726e",
   "metadata": {},
   "source": [
    "We can have a look at the latent space represented in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 38-50 solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 51- solutions/XOR_NN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1103949",
   "metadata": {},
   "source": [
    "## Extra : hyper-parameters\n",
    "\n",
    "\n",
    "**Question:** which hyper-parameters of our model can you identify?\n",
    "\n",
    "Answer:\n",
    "\n",
    " - model architecture (number/type of layers, activation functions, layer size,...)\n",
    " - optimizer (optimizer itself + learning rate, momentum,...)\n",
    " - batch size\n",
    " - number of epochs\n",
    "\n",
    "\n",
    "Hyper-parameters may be tuned using the classical tools of ML.\n",
    "\n",
    "We present here an adaptation of the [Ray tune tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) present on pytorch website, but other algorithms, such as those proposed in [sklearn](https://scikit-learn.org/stable/modules/grid_search.html), or with [hyperopt](http://hyperopt.github.io/hyperopt/), would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163c829-9dc4-4ac8-a948-6a13911f5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you are on google colab you will have to run the following\n",
    "#!pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a046e-d51c-4193-b5f2-d32af0ebb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369130-46ec-475d-9cca-9449c77ae150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf110d4a-8d0f-4cee-bb83-c7b7579e17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(dataloader, model, loss_fn, optimizer , echo = True):\n",
    "    \n",
    "    size = len(dataloader.dataset) # how many batches do we have\n",
    "    model.train() #     Sets the module in training mode.\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader): # for each batch\n",
    "        X, y = X.to(device), y.to(device) # send the data to the GPU or whatever device you use for training\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)              # prediction for the model -> forward pass\n",
    "        loss = loss_fn(pred.squeeze(), y)      # loss function from these prediction\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()              # backward propagation \n",
    "        #                            https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "        #                            https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "        \n",
    "        optimizer.step()             \n",
    "        optimizer.zero_grad()        # reset the gradients\n",
    "                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "\n",
    "        if echo:\n",
    "            current =  (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    # return the last batch loss:\n",
    "    return loss.item()\n",
    "\n",
    "def valid_func(dataloader, model, loss_fn, echo = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #     Sets the module in evaluation mode\n",
    "    valid_loss, correct = 0, 0\n",
    "    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y.unsqueeze(1)).item()  ## accumulating the loss function over the batches\n",
    "            \n",
    "            correct += ((pred>0.5).float() == y.unsqueeze(1)).float().sum().item() ## counting number of true predictions\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    if echo:\n",
    "        print(f\"Valid Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "    ## return the average loss / batch\n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_LRdeep(config, train_dataloader, valid_dataloader):\n",
    "    \n",
    "    model = deepLR_neuralNet( input_dim = 2 , hidden_dim = config['hidden_dim'] ).to(device)\n",
    "    \n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), \n",
    "                                lr=config['lr'] ,              # setup for hyper-param optimization\n",
    "                                momentum = config['momentum']) # setup for hyper-param optimization\n",
    "\n",
    "    epochs = 50\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_losses.append( train_func(train_dataloader, model, loss_fn, optimizer, echo=False) )\n",
    "        valid_loss = valid_func(valid_dataloader, model, loss_fn , echo = False)\n",
    "    \n",
    "        train.report( {\"loss\": valid_loss} ) # you may have to change this line to tune.report( {\"loss\": valid_loss} ) for some version of ray[tune]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1467ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    'momentum': tune.uniform(1e-3, 1-1e-3),\n",
    "    'hidden_dim': tune.randint(1, 20)\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=100,\n",
    "        grace_period=10,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    partial(train_LRdeep, \n",
    "            train_dataloader=train_dataloader ,\n",
    "            valid_dataloader=valid_dataloader),\n",
    "    resources_per_trial={\"cpu\": 4}, # change this to accomodate the resources at your disposal (# cpus, # gpus, ...)\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    checkpoint_at_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287b99a-b054-44ef-9e82-6800be3086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_config(metric = \"loss\" , mode = \"min\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8b9f1",
   "metadata": {},
   "source": [
    "## Extra : handling initialization to ease optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a9386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## second correction: initialization reduces optimization problems\n",
    "class deepLR_neuralNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self , input_dim = 2 , hidden_dim=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( nn.Linear(input_dim, hidden_dim),          # Linear layer\n",
    "                                     nn.ReLU(),                                 # ReLU layer\n",
    "                                     nn.Linear(hidden_dim, 1),                  # Linear layer\n",
    "                                     nn.Sigmoid()  # Non-linear activation\n",
    "                                   )\n",
    "        nn.init.xavier_normal_(self.layers[0].weight, \n",
    "                                         gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(self.layers[2].weight, \n",
    "                                         gain=nn.init.calculate_gain('sigmoid'))\n",
    "    def forward(self, x):  # Forward pass\n",
    "        proba = self.layers(x) \n",
    "        ## NB: here, the input  of the Sigmoid layer are logits\n",
    "        ##           the output of the Sigmoid layer are probas\n",
    "        return proba\n",
    "\n",
    "\n",
    "model = deepLR_neuralNet( input_dim = 2 ).to(device)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
